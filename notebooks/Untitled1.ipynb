{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a02e124-f10b-404b-b08c-bb3c6473d7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in c:\\users\\tonmo\\development\\ai\\env\\lib\\site-packages (0.11.7)\n",
      "Requirement already satisfied: pdfminer.six==20250506 in c:\\users\\tonmo\\development\\ai\\env\\lib\\site-packages (from pdfplumber) (20250506)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\tonmo\\development\\ai\\env\\lib\\site-packages (from pdfplumber) (10.4.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\tonmo\\development\\ai\\env\\lib\\site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\tonmo\\development\\ai\\env\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\tonmo\\development\\ai\\env\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (45.0.5)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\tonmo\\development\\ai\\env\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\tonmo\\development\\ai\\env\\lib\\site-packages (from cffi>=1.14->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1ed4c95-be2c-43b0-a99c-a438693b4a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document (./GoldmanSachs__2024-annual-report.pdf), has [<Page:1>, <Page:2>, <Page:3>, <Page:4>, <Page:5>, <Page:6>, <Page:7>, <Page:8>, <Page:9>, <Page:10>, <Page:11>, <Page:12>, <Page:13>, <Page:14>, <Page:15>, <Page:16>, <Page:17>, <Page:18>, <Page:19>, <Page:20>, <Page:21>, <Page:22>, <Page:23>, <Page:24>, <Page:25>, <Page:26>, <Page:27>, <Page:28>, <Page:29>, <Page:30>, <Page:31>, <Page:32>, <Page:33>, <Page:34>, <Page:35>, <Page:36>, <Page:37>, <Page:38>, <Page:39>, <Page:40>, <Page:41>, <Page:42>, <Page:43>, <Page:44>, <Page:45>, <Page:46>, <Page:47>, <Page:48>, <Page:49>, <Page:50>, <Page:51>, <Page:52>, <Page:53>, <Page:54>, <Page:55>, <Page:56>, <Page:57>, <Page:58>, <Page:59>, <Page:60>, <Page:61>, <Page:62>, <Page:63>, <Page:64>, <Page:65>, <Page:66>, <Page:67>, <Page:68>, <Page:69>, <Page:70>, <Page:71>, <Page:72>, <Page:73>, <Page:74>, <Page:75>, <Page:76>, <Page:77>, <Page:78>, <Page:79>, <Page:80>, <Page:81>, <Page:82>, <Page:83>, <Page:84>, <Page:85>, <Page:86>, <Page:87>, <Page:88>, <Page:89>, <Page:90>, <Page:91>, <Page:92>, <Page:93>, <Page:94>, <Page:95>, <Page:96>, <Page:97>, <Page:98>, <Page:99>, <Page:100>, <Page:101>, <Page:102>, <Page:103>, <Page:104>, <Page:105>, <Page:106>, <Page:107>, <Page:108>, <Page:109>, <Page:110>, <Page:111>, <Page:112>, <Page:113>, <Page:114>, <Page:115>, <Page:116>, <Page:117>, <Page:118>, <Page:119>, <Page:120>, <Page:121>, <Page:122>, <Page:123>, <Page:124>, <Page:125>, <Page:126>, <Page:127>, <Page:128>, <Page:129>, <Page:130>, <Page:131>, <Page:132>, <Page:133>, <Page:134>, <Page:135>, <Page:136>, <Page:137>, <Page:138>, <Page:139>, <Page:140>, <Page:141>, <Page:142>, <Page:143>, <Page:144>, <Page:145>, <Page:146>, <Page:147>, <Page:148>, <Page:149>, <Page:150>, <Page:151>, <Page:152>, <Page:153>, <Page:154>, <Page:155>, <Page:156>, <Page:157>, <Page:158>, <Page:159>, <Page:160>, <Page:161>, <Page:162>, <Page:163>, <Page:164>, <Page:165>, <Page:166>, <Page:167>, <Page:168>, <Page:169>, <Page:170>, <Page:171>, <Page:172>, <Page:173>, <Page:174>, <Page:175>, <Page:176>, <Page:177>, <Page:178>, <Page:179>, <Page:180>, <Page:181>, <Page:182>, <Page:183>, <Page:184>, <Page:185>, <Page:186>, <Page:187>, <Page:188>, <Page:189>, <Page:190>, <Page:191>, <Page:192>, <Page:193>, <Page:194>, <Page:195>, <Page:196>, <Page:197>, <Page:198>, <Page:199>, <Page:200>, <Page:201>, <Page:202>, <Page:203>, <Page:204>, <Page:205>, <Page:206>, <Page:207>, <Page:208>, <Page:209>, <Page:210>, <Page:211>, <Page:212>, <Page:213>, <Page:214>, <Page:215>, <Page:216>, <Page:217>, <Page:218>, <Page:219>, <Page:220>, <Page:221>, <Page:222>, <Page:223>, <Page:224>, <Page:225>, <Page:226>, <Page:227>, <Page:228>, <Page:229>, <Page:230>, <Page:231>, <Page:232>, <Page:233>, <Page:234>, <Page:235>, <Page:236>, <Page:237>, <Page:238>, <Page:239>, <Page:240>, <Page:241>, <Page:242>, <Page:243>, <Page:244>, <Page:245>, <Page:246>, <Page:247>, <Page:248>, <Page:249>, <Page:250>, <Page:251>, <Page:252>, <Page:253>, <Page:254>, <Page:255>, <Page:256>, <Page:257>, <Page:258>, <Page:259>, <Page:260>, <Page:261>, <Page:262>, <Page:263>, <Page:264>, <Page:265>, <Page:266>, <Page:267>] pages.\n",
      "Text successfully extracted for NLP analysis.\n",
      "THE GOLDMAN SACHS GROUP, INC.\n",
      "Annual Report\n",
      "2024\n",
      "1\n",
      "Fellow shareholders:\n",
      "In my last letter, I wrote that 2023\n",
      "was a year of execution, where we\n",
      "made important progress on our\n",
      "strategy and put the firm in a stronger\n",
      "position going forward. I am pleased\n",
      "to report that in 2024, we saw the\n",
      "benefits of our continued investment\n",
      "in our franchise and our people, which\n",
      "helped us serve our clients with\n",
      "excellence and deliver strong results\n",
      "for shareholders.\n",
      "In 2024, we increased our net revenues by 16 perc\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts all text from a PDF file.\n",
    "    Args:\n",
    "        pdf_path (str): The path to the PDF file.\n",
    "    Returns:\n",
    "        str: The concatenated text from all pages of the PDF.\n",
    "    \"\"\"\n",
    "    full_text = ''\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            print(f'The document ({pdf_path}), has {pdf.pages} pages.')\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text: \n",
    "                    full_text += page_text + \"\\n\" \n",
    "    except Exception as e:\n",
    "        print(f'Error extracting text from {pdf_path}: {e}')\n",
    "        return None\n",
    "    return full_text\n",
    "\n",
    "pdf_file = \"./GoldmanSachs__2024-annual-report.pdf\"  \n",
    "extracted_content = extract_text_from_pdf(pdf_file)\n",
    "\n",
    "if extracted_content:\n",
    "    print(\"Text successfully extracted for NLP analysis.\")\n",
    "    print(extracted_content[:500])\n",
    "else:\n",
    "    print(\"Failed to extract text.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feb59a0e-2e35-4598-add2-9c27dc2f7e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Here's one:\n",
      "\n",
      "What do you call a fake noodle?\n",
      "\n",
      "(Wait for it...)\n",
      "\n",
      "An impasta!\n",
      "\n",
      "Hope that made you laugh! Do you want to hear another one?\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "OLLAMA_URL = \"http://localhost:11434\"\n",
    "def chat_with_llama3(prompt, model=\"llama3.2\", stream=False):\n",
    "    \"\"\"\n",
    "    Send a prompt to Llama3 and get response\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The prompt to send\n",
    "        model (str): Model name (default: llama3)\n",
    "        stream (bool): Whether to stream response\n",
    "    \n",
    "    Returns:\n",
    "        str: The response from the model\n",
    "    \"\"\"\n",
    "    url = f\"{OLLAMA_URL}/api/generate\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": stream\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, json=payload)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            if stream:\n",
    "                # Handle streaming response\n",
    "                full_response = \"\"\n",
    "                for line in response.iter_lines():\n",
    "                    if line:\n",
    "                        chunk = json.loads(line.decode('utf-8'))\n",
    "                        if 'response' in chunk:\n",
    "                            print(chunk['response'], end='', flush=True)\n",
    "                            full_response += chunk['response']\n",
    "                        if chunk.get('done', False):\n",
    "                            break\n",
    "                print()  # New line at the end\n",
    "                return full_response\n",
    "            else:\n",
    "                # Handle non-streaming response\n",
    "                result = response.json()\n",
    "                return result.get('response', 'No response received')\n",
    "        else:\n",
    "            return f\"Error: {response.status_code} - {response.text}\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Test the function\n",
    "response = chat_with_llama3(\"Hello! Can you tell me a joke?\")\n",
    "print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b799893-79e2-431a-98bf-23c18d9ec9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16617\n"
     ]
    }
   ],
   "source": [
    "def chunk_texts(root_text: str, chunk_size:int = 10, overlap:int = 0, split_on_whitespace_only=True) -> list[str]:\n",
    "    chunks: list[str] = list()\n",
    "    index: int = 0\n",
    "    len_whole_text: int = len(root_text)\n",
    "    if len_whole_text == 0:\n",
    "        raise Exception('No Root text sent for chunking into chunk_texts()')\n",
    "    while index < len_whole_text:\n",
    "        if split_on_whitespace_only:\n",
    "            prev_whitespace = 0\n",
    "            left_index = index - overlap\n",
    "            while left_index >= 0:\n",
    "                if root_text[left_index] == \" \":\n",
    "                    prev_whitespace = left_index\n",
    "                    break\n",
    "                left_index -= 1\n",
    "            next_whitespace = root_text.find(\" \", index + chunk_size)\n",
    "            if next_whitespace == -1:\n",
    "                next_whitespace = len_whole_text\n",
    "            chunks.append(root_text[prev_whitespace:next_whitespace].strip())\n",
    "            index = next_whitespace + 1\n",
    "        else:\n",
    "            start = max(0, index - overlap + 1)\n",
    "            end = min(index + chunk_size + overlap, len_whole_text)\n",
    "            chunks.append(root_text[start:end].strip())\n",
    "            index += chunk_size\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_texts(extracted_content, chunk_size = 50)\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c899c1af-5bb1-4f50-b768-638713ffdef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "#!pip uninstall keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0398be1b-1cca-4289-9a5b-add2914b1703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "#!pip uninstall tf-keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487f38a8-2812-4304-a0df-e85d2c94c2de",
   "metadata": {},
   "source": [
    "A wide range of models are available on the Hugging Face Model Hub. Popular choices include:\n",
    "all-MiniLM-L6-v2: Produces 384-dimensional embeddings. It offers a good balance between performance and speed.\n",
    "all-mpnet-base-v2: Generates 768-dimensional embeddings and is a high-performing base model for semantic search.\n",
    "mixedbread-ai/mxbai-embed-large-v1: A high-performance model that produces 1024-dimensional embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab55caad-7a18-4323-b53f-182a8bcd3f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec04329b235d49e8a2fa033d9f11d954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tonmo\\Development\\AI\\env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\tonmo\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "751889f8c9ce470e8d59a54fc9d2a2c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd05a1465a0744089e27a4ffdc68dc14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f2d27975a8e48a38f4b9e853071a5c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e409095baa044e279cc43876309f6851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85e584ba2bfa4e42871d9f541d637e22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b5d1243078647df8375954f2d37bfb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96647af3f896494384e4e0147f838d5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aed0e1bc49b4af88bc5acbee76bd28e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93010a77e5464b9b9a8175f180134d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c2afadf1c2f4e639b2684a97430b7a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#note: keras 3 is incompatibel with sentence_gransformers and raised an exception\n",
    "#pip install tf-keras was executed in response\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a pre-trained model\n",
    "#model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "embeddings = model.encode(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71b1042a-9212-4c76-81ac-089cd259805c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Embeddings is 16617, with the number of dims for first embeddins being 768\n"
     ]
    }
   ],
   "source": [
    "print(f'Num of Embeddings is {len(embeddings)}, with the number of dims for first embeddins being {len(embeddings[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eefc8f7c-8691-4a41-9e55-d027defa885d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: neo4j in c:\\users\\tonmo\\development\\ai\\env\\lib\\site-packages (5.28.2)\n",
      "Requirement already satisfied: pytz in c:\\users\\tonmo\\development\\ai\\env\\lib\\site-packages (from neo4j) (2024.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee7c94a9-65dd-4be7-b98a-2dcc8e29d41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Connect to Neo4j over Bolt\n",
    "uri = \"bolt://localhost:7687\"\n",
    "username = \"neo4j\"\n",
    "password = \"password1\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(username, password))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d8daf40-9de0-4e95-8408-66b00634f7df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<neo4j._sync.driver.BoltDriver at 0x193f47e3380>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6283016-5136-4b04-97e9-07e36253a159",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
